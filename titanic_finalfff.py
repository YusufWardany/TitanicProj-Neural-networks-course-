# -*- coding: utf-8 -*-
"""Titanic_finalFFF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rShm_nt9A9dNhoUA-1An7wwYtnzj2KLI

# Project phasese:

<a id="Explanition of dataset"></a> <br>
# **1. Explanition of dataset:**
<h3> The data have 891 entries on train dataset and 418 on test dataset</h3>
- 10 columns in train_csv and 9 columns in train_test

<h2>Competition Description: </h2>
The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.

One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.

In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.

<h3>Data Dictionary</h3><br>
Variable	Definition	Key<br>
<b>survival</b>	Survival	0 = No, 1 = Yes<br>
<b>pclass</b>	Ticket class	1 = 1st, 2 = 2nd, 3 = 3rd<br>
<b>sex</b>	Sex	<br>
<b>Age</b>	Age in years	<br>
<b>sibsp</b>	# of siblings / spouses aboard the Titanic	<br>
<b>parch</b>	# of parents / children aboard the Titanic	<br>
<b>ticket</b>	Ticket number	<br>
<b>fare</b>	Passenger fare	<br>
<b>cabin</b>	Cabin number	<br>
<b>embarked	</b>Port of Embarkation	C = Cherbourg, Q = Queenstown, S = Southampton<br>
<h3>Variable Notes</h3><br>
<b>pclass: </b>A proxy for socio-economic status (SES)<br>
1st = Upper<br>
2nd = Middle<br>
3rd = Lower<br>
<b>age: </b>Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br>
<b>sibsp:</b> The dataset defines family relations in this way...<br>
- <b>Sibling </b>= brother, sister, stepbrother, stepsister<br>
- <b>Spouse </b>= husband, wife (mistresses and fianc√©s were ignored)<br>

<b>parch: </b>The dataset defines family relations in this way...<br>
- <b>Parent</b> = mother, father<br>
- <b>Child </b>= daughter, son, stepdaughter, stepson<br>

Some children travelled only with a nanny, therefore parch=0 for them.<br>

I am using the beapproachs as possible but if you think I can do anything another best way, please, let me know.

<a id="Librarys"></a> <br>
# **2. Librarys:**
"""

# Commented out IPython magic to ensure Python compatibility.
#This librarys is to work with matrices
import pandas as pd
# This librarys is to work with vectors
import numpy as np
# This library is to create some graphics algorithmn
import seaborn as sns
# to render the graphs
import matplotlib.pyplot as plt
# import module to set some ploting parameters
from matplotlib import rcParams
# Library to work with Regular Expressions
import re

# This function makes the plot directly on browser
# %matplotlib inline

# Seting a universal figure size
rcParams['figure.figsize'] = 10,8

#!pip install seaborn

# Importing train dataset
df_train = pd.read_csv("train.csv")

# Importing test dataset
df_test = pd.read_csv("test.csv")

"""<a id="Known"></a> <br>
# **3. First look at the data:**

I will start looking the type and informations of the datasets
"""

#Looking data format and types
print(df_train.info())

# printing test info()
print(df_test.info())

#Some Statistics
df_train.describe()

#Take a look at the data
(df_train.head())

"""<a id="Known"></a> <br>
# **4. Exploring the data:**

<h2>To try a new approach in the data, instend of removing name from attribute without using it we expract tittle of persons from it
"""

#Looking how the data is and searching for a re patterns
df_train["Name"].head()

#GettingLooking the prefix of all Passengers
df_train['Title'] = df_train.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\.', x).group(1))

#defining the figure size of our graphic
plt.figure(figsize=(12,5))

#Plotting the result
sns.countplot(x='Title', data=df_train, palette="hls")
plt.xlabel("Title", fontsize=16) #seting the xtitle and size
plt.ylabel("Count", fontsize=16) # Seting the ytitle and size
plt.title("Title Name Count", fontsize=20)
plt.xticks(rotation=45)
plt.show()

#Doing the same on df_test with regular expressions
df_test['Title'] = df_test.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\.', x).group(1))

"""## Grouping some titles and ploting the results"""

# identify the social status of each title
Title_Dictionary = {
        "Capt":       "Officer",
        "Col":        "Officer",
        "Major":      "Officer",
        "Dr":         "Officer",
        "Rev":        "Officer",
        "Jonkheer":   "Royalty",
        "Don":        "Royalty",
        "Sir" :       "Royalty",
        "the Countess":"Royalty",
        "Dona":       "Royalty",
        "Lady" :      "Royalty",
        "Mme":        "Mrs",
        "Ms":         "Mrs",
        "Mrs" :       "Mrs",
        "Mlle":       "Miss",
        "Miss" :      "Miss",
        "Mr" :        "Mr",
        "Master" :    "Master"
                   }

# we map each title to correct category
df_train['Title'] = df_train.Title.map(Title_Dictionary)
df_test['Title'] = df_test.Title.map(Title_Dictionary)

"""## Title grouped here we conclude that tittle affect Chances to survive
## as Master 0.575000
## as Miss   0.701087
## as Royalty 0.500000
"""

#printing the chance to survive by each title
print("Chances to survive based on titles: ")
print(df_train.groupby("Title")["Survived"].mean())
# figure size
plt.figure(figsize=(12,5))

#Plotting the count of title by Survived or not category
sns.countplot(x='Title', data=df_train, palette="hls",
              hue="Survived")
plt.xlabel("Titles", fontsize=16)
plt.ylabel("Count", fontsize=16)
plt.title("Title Grouped Count", fontsize=20)
plt.xticks(rotation=45)
plt.show()

"""It's interesting... Children's and ladys first, huh?

```
# This is formatted as code
```

<h1>  handle the Age variable that has a high number of NaN's, using some columns to correctly input he missing Age's
"""

#First we look my distribuition without NaN's
#I will create a df to look distribuition
age_high_zero_died = df_train[(df_train["Age"] > 0) &
                              (df_train["Survived"] == 0)]
age_high_zero_surv = df_train[(df_train["Age"] > 0) &
                              (df_train["Survived"] == 1)]

#figure size
plt.figure(figsize=(10,5))
# Ploting the 2 variables that we create and compare the two
sns.distplot(age_high_zero_surv["Age"], bins=24, color='g')# green to which sur
sns.distplot(age_high_zero_died["Age"], bins=24, color='r')# red to which die
plt.title("Distribuition and density by Age",fontsize=20)
plt.xlabel("Age",fontsize=15)
plt.ylabel("Distribuition Died and Survived",fontsize=15)
plt.show()

#Let's group the median age by sex, pclass and title, to have any idea and see if there is relation
age_group = df_train.groupby(["Sex","Pclass","Title"])["Age"]

#printing the variabe that we created by median
print(age_group.median())

"""

<b> example: </b> an male in 2 class that is a Officer the median Age is 42. <br>
And we will use that to complete the missing data
"""

#inputing the values on Age Na's
# using the groupby to transform this variables
df_train.loc[df_train.Age.isnull(), 'Age'] = df_train.groupby(['Sex','Pclass','Title']).Age.transform('median')

# printing the total of nulls in Age Feature
print(df_train["Age"].isnull().sum())

"""# result after adding missing values of age"""

#Let's see the result of the inputation
#seting the figure size
plt.figure(figsize=(12,5))

#ploting again the Age Distribuition after the transformation in our dataset
sns.distplot(df_train["Age"], bins=24)
plt.title("Distribuition and density by Age after filling missing values")
plt.xlabel("Age")
plt.show()

"""Now let's categorize them"""

#separate by survivors or not

# figure size
plt.figure(figsize=(12,5))

# using facetgrid that is a great way to get information of our dataset
# Changed 'size' to 'height'
g = sns.FacetGrid(df_train, col='Survived', height=5)
g = g.map(sns.distplot, "Age")
plt.show()

#df_train.Age = df_train.Age.fillna(-0.5)

#creating the intervals that we need to cut each range of ages
interval = (0, 5, 12, 18, 25, 35, 60, 120)

#Seting the names that we want use to the categorys
cats = ['babies', 'Children', 'Teen', 'Student', 'Young', 'Adult', 'Senior']

# Applying the pd.cut and using the parameters that we created
df_train["Age_cat"] = pd.cut(df_train.Age, interval, labels=cats)

# Printing the new Category
df_train["Age_cat"].head()

#Do the same to test dataset
interval = (0, 5, 12, 18, 25, 35, 60, 120)

#same as the other df train
cats = ['babies', 'Children', 'Teen', 'Student', 'Young', 'Adult', 'Senior']

# same that we used above in df train
df_test["Age_cat"] = pd.cut(df_test.Age, interval, labels=cats)

"""Now it look's better and clearly"""

#Describe of categorical Age

# Using pd.crosstab to understand the Survived rate by Age Category's
print(pd.crosstab(df_train.Age_cat, df_train.Survived))

#Seting the figure size
plt.figure(figsize=(12,10))

#Plotting the result
plt.subplot(2,1,1)
# The first positional argument should be 'x'
sns.countplot(x="Age_cat", data=df_train, hue="Survived", palette="hls")
plt.ylabel("Count", fontsize=18)
plt.xlabel("Age Categorys", fontsize=18)
plt.title("Age Distribution ", fontsize=20)

plt.subplot(2,1,2)
sns.swarmplot(x='Age_cat',y="Fare",data=df_train,
              hue="Survived", palette="hls", )
plt.ylabel("Fare Distribution", fontsize=18)
plt.xlabel("Age Categorys", fontsize=18)
plt.title("Fare Distribution by Age Categorys ", fontsize=20)

plt.subplots_adjust(hspace = 0.5, top = 0.9)

plt.show()

"""## cross our Pclass with the Age_cat
We will aggregate than to get the mean of Fare by each category pair

"""

Age_fare = ['Pclass', 'Age_cat'] #seting the desired
cm = sns.light_palette("green", as_cmap=True)
pd.crosstab(df_train[Age_fare[0]], df_train[Age_fare[1]],
            values=df_train['Fare'], aggfunc=['mean']).style.background_gradient(cmap = cm)

"""- Very interesting. We can see that babies has the highest mean value.

### Looking the Fare distribuition to survivors and not survivors
### we conclude from that low fare have very high change of not Survived
"""

# Seting the figure size
plt.figure(figsize=(12,5))

# Understanding the Fare Distribuition
sns.distplot(df_train[df_train.Survived == 0]["Fare"],
             bins=50, color='r')
sns.distplot(df_train[df_train.Survived == 1]["Fare"],
             bins=50, color='g')
plt.title("Fare Distribuition by Survived", fontsize=20)
plt.xlabel("Fare", fontsize=15)
plt.ylabel("Density",fontsize=15)
plt.show()

"""<br>
Description of Fare variable<br>
- Min: 0<br>
- Median: 14.45<br>
- Mean: 32.20<br>
- Max: 512.32<br>
- Std: 49.69<br>

<h3> create a categorical variable to treat the Fare expend</h3><br>
I will use the same technique used in Age but now I will use the quantiles to binning


"""

#Filling the NA's with -0.5
df_train.Fare = df_train.Fare.fillna(-0.5)

#intervals to categorize
quant = (-1, 0, 8, 15, 31, 600)

#Labels without input values
label_quants = ['NoInf', 'quart_1', 'quart_2', 'quart_3', 'quart_4']

#doing the cut in fare and puting in a new column
df_train["Fare_cat"] = pd.cut(df_train.Fare, quant, labels=label_quants)

#Description of transformation
print(pd.crosstab(df_train.Fare_cat, df_train.Survived))

plt.figure(figsize=(12,5))

#Plotting the new feature
sns.countplot(x="Fare_cat", hue="Survived", data=df_train, palette="hls")
plt.title("Count of survived x Fare expending",fontsize=20)
plt.xlabel("Fare Cat",fontsize=15)
plt.ylabel("Count",fontsize=15)

plt.show()

# Replicate the same to df_test
df_test.Fare = df_test.Fare.fillna(-0.5)

quant = (-1, 0, 8, 15, 31, 1000)
label_quants = ['NoInf', 'quart_1', 'quart_2', 'quart_3', 'quart_4']

df_test["Fare_cat"] = pd.cut(df_test.Fare, quant, labels=label_quants)

"""<h2>To complete this part, I will now work on "Names"
"""

#Now lets drop the variable Fare, Age and ticket that is irrelevant now
del df_train["Fare"]
del df_train["Ticket"]
del df_train["Age"]
del df_train["Cabin"]
del df_train["Name"]

#same in df_test
del df_test["Fare"]
del df_test["Ticket"]
del df_test["Age"]
del df_test["Cabin"]
del df_test["Name"]

#Looking the result of transformations
df_train.head()

"""<h1> explore on dataset

Now, lets start explore the data
"""

# Let see how many people die or survived
print("Total of Survived or not: ")
print(df_train.groupby("Survived")["PassengerId"].count())
plt.figure(figsize=(12,5))
sns.countplot(x="Survived", data=df_train,palette="hls")
plt.title('Total Distribuition by survived or not', fontsize=22)
plt.xlabel('Target Distribuition', fontsize=18)
plt.ylabel('Count', fontsize=18)
plt.show()

print(pd.crosstab(df_train.Survived, df_train.Sex))

plt.figure(figsize=(12,5))
sns.countplot(x="Sex", data=df_train, hue="Survived",palette="hls")
plt.title('Sex Distribuition by survived or not', fontsize=20)
plt.xlabel('Sex Distribuition',fontsize=17)
plt.ylabel('Count', fontsize=17)

plt.show()

"""<h2>We can look that % dies to mens are much higher than female

<h1>finding some relation between Pclass and Embarked to see if might have some information to build the model
we notice that most frequent Embarked is S
"""

# Distribuition by class
print(pd.crosstab(df_train.Pclass, df_train.Embarked))
plt.figure(figsize=(12,5))
sns.countplot(x="Embarked", data=df_train, hue="Pclass",palette="hls")
plt.title('Embarked x Pclass Count', fontsize=20)
plt.xlabel('Embarked with PClass',fontsize=17)
plt.ylabel('Count', fontsize=17)

plt.show()

#lets input the NA's with the highest frequency
df_train["Embarked"] = df_train["Embarked"].fillna('S')

# Exploring Survivors vs Embarked
print(pd.crosstab(df_train.Survived, df_train.Embarked))

plt.figure(figsize=(12,5))

sns.countplot(x="Embarked", data=df_train, hue="Survived",palette="hls")
plt.title('Class Distribuition by survived or not',fontsize=20)
plt.xlabel('Embarked',fontsize=17)
plt.ylabel('Count', fontsize=17)

plt.show()

# Exploring Survivors vs Pclass
print(pd.crosstab(df_train.Survived, df_train.Pclass))

plt.figure(figsize=(12,5))

sns.countplot(x="Pclass", data=df_train, hue="Survived",palette="hls")
plt.xlabel('PClass',fontsize=17)
plt.ylabel('Count', fontsize=17)
plt.title('Class Distribuition by Survived or not', fontsize=20)

plt.show()

"""<b>Looking the graphs, is clear that 3st class and Embarked at Southampton have a high probabilities to not survive</b>

the Sibsp and prch variables
"""

# Replace 'factorplot' with 'catplot'
g = sns.catplot(x="SibSp", y="Survived", data=df_train,
                kind="bar", height=5, aspect=1.6, palette="hls")
g.set_ylabels("Probability(Survive)", fontsize=15)
g.set_xlabels("SibSp Number", fontsize=15)

plt.show()

"""With "1 or 2" siblings/spouses have more chance to survived we con that high prop to serived

We can see a high standard deviation in the survival with 3 parents/children person's <br>
Also that small families (1~2) have more chance to survival than single or big families

So to Finish our exploration I will create a new column to with familiees size
"""

# Explore Parch feature vs Survived
# Replace 'factorplot' with 'catplot'
g = sns.catplot(x="Parch", y="Survived", data=df_train, kind="bar", height=6, palette="hls")
g.set_ylabels("survival probability")
plt.show() # Add plt.show() to display the plot

#Create a new column and sum the Parch + SibSp + 1 that refers the people self
df_train["FSize"] = df_train["Parch"] + df_train["SibSp"] + 1
df_test["FSize"] = df_test["Parch"] + df_test["SibSp"] + 1

print(pd.crosstab(df_train.FSize, df_train.Survived))

# Replace 'factorplot' with 'catplot' and adjust parameters
sns.catplot(x="FSize", y="Survived", data=df_train, kind="bar", height=6, aspect=1.6, palette="hls")
plt.show() # Add plt.show() to display the plot

del df_train["SibSp"]
del df_train["Parch"]

del df_test["SibSp"]
del df_test["Parch"]

"""OK, its might be enough to start with the preprocess and builting the model

<a id="Preprocess"></a> <br>
# **5. Preprocessing :**
"""

df_train.head()

"""Now we might have information enough to think about the model structure"""

df_train = pd.get_dummies(df_train, columns=["Sex","Embarked","Age_cat","Fare_cat","Title"],\
                          prefix=["Sex","Emb","Age","Fare","Prefix"], drop_first=True)

df_test = pd.get_dummies(df_test, columns=["Sex","Embarked","Age_cat","Fare_cat","Title"],\
                         prefix=["Sex","Emb","Age","Fare","Prefix"], drop_first=True)

#Finallt, lets look the correlation of df_train
plt.figure(figsize=(15,12))
plt.title('Correlation of Features for Train Set')
sns.heatmap(df_train.astype(float).corr(),vmax=1.0,  annot=True)
plt.show()

df_train.shape

train = df_train.drop(["Survived","PassengerId"],axis=1)
train_ = df_train["Survived"]

test_ = df_test.drop(["PassengerId"],axis=1)

X_train = train.values
y_train = train_.values

X_test = test_.values
X_test = X_test.astype(np.float64, copy=False)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

"""<a id="Model"></a> <br>
# **6. Modelling : **

<h3>Titanic survivors prediction: <br>
a binary classification example</h3>
Two-class classification, or binary classification, may be the most widely applied kind of machine-learning problem.
"""

import warnings
warnings.filterwarnings('ignore')

#!pip install graphviz

from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
import keras
from keras.optimizers import SGD
import graphviz

"""<h1>building of a neural network: </h1>

phases of neural :
- Layers, which are combined into a network (or model)
- The input data and corresponding targets
- The loss function, which defines the feedback signal used for learning
- The optimizer, which determines how learning proceeds

## model 1
"""

# Creating the model
model = Sequential()

# Inputing the first layer with input dimensions
model.add(Dense(18,
                activation='relu',
                input_dim=20,
                kernel_initializer='uniform'))
#The argument being passed to each Dense layer (18) is the number of hidden units of the layer.
# A hidden unit is a dimension in the representation space of the layer.

#Stacks of Dense layers with relu activations can solve a wide range of problems
#(including sentiment classification), and you‚Äôll likely use them frequently.

# Adding an Dropout layer to previne from overfitting
model.add(Dropout(0.50))

#adding second hidden layer
model.add(Dense(60,
                kernel_initializer='uniform',
                activation='relu'))

# Adding another Dropout layer
model.add(Dropout(0.50))

# adding the output layer that is binary [0,1]
model.add(Dense(1,
                kernel_initializer='uniform',
                activation='sigmoid'))
#With such a scalar sigmoid output on a binary classification problem, the loss
#function you should use is binary_crossentropy

#Visualizing the model
model.summary()

"""using layers with relu activations can solve a wide range of problems (including sentiment classification), and you‚Äôll likely use them frequently.

Finally, we need to choose a loss function and an optimizer.
"""

#Creating an Stochastic Gradient Descent
sgd = SGD(learning_rate = 0.01, momentum = 0.9) # Changed lr to learning_rate

# Compiling our model
model.compile(optimizer = sgd,
                   loss = 'binary_crossentropy',
                   metrics = ['accuracy'])
#optimizers list
#optimizers['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']

# Fitting the ANN to the Training set
model.fit(X_train, y_train,
               batch_size = 60,
               epochs = 30, verbose=2)

"""Because you‚Äôre facing a binary classification problem and the output of your network is a probability (you end your network with a single-unit layer with a sigmoid activation), it‚Äôs best to use the <i>binary_crossentropy</i> loss.

## Predicting X_test
"""

y_preds = model.predict(X_test)

submission = pd.read_csv("gender_submission.csv", index_col='PassengerId')
submission['Survived'] = y_preds.astype(int)
submission.to_csv('TitanicKNN.csv')

"""<h1>Evaluating the model</h1>"""

scores = model.evaluate(X_train, y_train, batch_size=30)
print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))

"""Not bad result to a simple model! Let's now verify the validation of our model, to see and understand the learning curve

<a id="Validation"></a> <br>
# **7. Validation: **
"""

# Fit the model
history = model.fit(X_train, y_train, validation_split=0.20,
                    epochs=180, batch_size=10, verbose=0)

"""Let's look this keys values further"""

# summarizing historical accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

""" this is a overffiting"""

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

y_pred = model.predict(X_test)

"""## modele 2"""

from keras.regularizers import l2

# ... (Rest of your code) ...

# Creating the model with L2 regularization
model_l2 = Sequential()

# Input layer with L2 regularization
model_l2.add(Dense(18, activation='relu', input_dim=20, kernel_initializer='uniform', kernel_regularizer=l2(0.01)))
model_l2.add(Dropout(0.5))

# Second hidden layer with L2 regularization
model_l2.add(Dense(60, kernel_initializer='uniform', activation='relu', kernel_regularizer=l2(0.01)))
model_l2.add(Dropout(0.5))

# Output layer
model_l2.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))

# Recreate the optimizer for the new model
sgd = SGD(learning_rate=0.01, momentum=0.9) # Create a new SGD instance

# Compile the model with the new optimizer
model_l2.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])

# Fit the model
history_l2 = model_l2.fit(X_train, y_train, validation_split=0.20, epochs=180, batch_size=10, verbose=0)

# Plotting accuracy
plt.plot(history_l2.history['accuracy'])
plt.plot(history_l2.history['val_accuracy'])
plt.title('Model 1: L2 Regularization - Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Plotting loss
plt.plot(history_l2.history['loss'])
plt.plot(history_l2.history['val_loss'])
plt.title('Model 1: L2 Regularization - Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""mode 2

# model 3
"""

import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.callbacks import EarlyStopping
from keras.regularizers import l2
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
# Assuming X_train and y_train are your original training data
X_train_train, X_train_test, y_train_train, y_train_test = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42
)  # Split data

# Initialize the ANN
ann = Sequential()

# Input layer and first hidden layer with L2 regularization and dropout
ann.add(Dense(units=14, activation='relu', input_dim=X_train_train.shape[1], kernel_regularizer=l2(0.01)))
ann.add(Dropout(0.3))

# Second hidden layer with L2 regularization and dropout
ann.add(Dense(units=14, activation='relu', kernel_regularizer=l2(0.01)))
ann.add(Dropout(0.3))

# Output layer
ann.add(Dense(units=1, activation='sigmoid'))

# Compile the model
ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Fit the model with early stopping and store the history
history = ann.fit(X_train_train, y_train_train, batch_size=8, epochs=100,
                    validation_data=(X_train_test, y_train_test),
                    callbacks=[early_stopping])

# Evaluate the model on the training set
train_accuracy = ann.evaluate(X_train_train, y_train_train, verbose=0)[1]
print(f"Train Accuracy: {train_accuracy}")

# Evaluate the model on the test set
test_accuracy = ann.evaluate(X_train_test, y_train_test, verbose=0)[1]
print(f"Test Accuracy: {test_accuracy}")

# Plot training and validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training and validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()